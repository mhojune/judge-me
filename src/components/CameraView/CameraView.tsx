import { useEffect, useRef, useState } from 'react'
import { Camera } from '@mediapipe/camera_utils'
import { FaceMesh } from '@mediapipe/face_mesh'
import { drawConnectors, drawLandmarks } from '@mediapipe/drawing_utils'
import { FACEMESH_TESSELATION } from '@mediapipe/face_mesh'
import { useCamera } from '../../hooks/useCamera'
import './CameraView.css'

interface CameraViewProps {
  onFaceDetected?: (landmarks: any) => void
  fps?: number
  transcript?: string // ìŒì„± ì¸ì‹ í…ìŠ¤íŠ¸
}

/**
 * CameraView ì»´í¬ë„ŒíŠ¸
 * 
 * ì—­í• :
 * - ì‚¬ìš©ì ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ì„ ê°€ì ¸ì™€ í™”ë©´ì— í‘œì‹œ
 * - MediaPipe Face Meshë¥¼ ì‚¬ìš©í•œ ì–¼êµ´ ì¸ì‹
 * - ì–¼êµ´ ëœë“œë§ˆí¬ë¥¼ ìº”ë²„ìŠ¤ì— ê·¸ë¦¬ê¸°
 * - FPS ì œí•œì„ í†µí•œ ì„±ëŠ¥ ìµœì í™”
 */
export default function CameraView({ onFaceDetected, fps = 30, transcript = '' }: CameraViewProps) {
  const videoRef = useRef<HTMLVideoElement>(null)
  const canvasRef = useRef<HTMLCanvasElement>(null)
  const [isActive, setIsActive] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const { startCamera, stopCamera, isStreaming } = useCamera()

  useEffect(() => {
    if (!videoRef.current || !canvasRef.current) return

    const video = videoRef.current
    const canvas = canvasRef.current
    const ctx = canvas.getContext('2d')

    if (!ctx) return

    let faceMesh: FaceMesh | null = null
    let camera: Camera | null = null

    const initializeFaceMesh = async () => {
      faceMesh = new FaceMesh({
        locateFile: (file) => {
          return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
        },
      })

      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5,
      })

      faceMesh.onResults((results) => {
        if (ctx && canvas) {
          ctx.save()
          ctx.clearRect(0, 0, canvas.width, canvas.height)
          ctx.drawImage(results.image, 0, 0, canvas.width, canvas.height)

          if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
            const landmarks = results.multiFaceLandmarks[0]
            
            // ì–¼êµ´ ë©”ì‹œ ê·¸ë¦¬ê¸°
            drawConnectors(ctx, landmarks, FACEMESH_TESSELATION, {
              color: '#C0C0C070',
              lineWidth: 1,
            })
            
            // ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
            drawLandmarks(ctx, landmarks, {
              color: '#FF0000',
              radius: 1,
            })

            // ë¶€ëª¨ ì»´í¬ë„ŒíŠ¸ì— ì–¼êµ´ ê°ì§€ ì •ë³´ ì „ë‹¬
            onFaceDetected?.(landmarks)
          }
        }
      })

      // ì¹´ë©”ë¼ ì´ˆê¸°í™”
      camera = new Camera(video, {
        onFrame: async () => {
          if (faceMesh) {
            await faceMesh.send({ image: video })
          }
        },
        width: 1280,
        height: 720,
      })

      try {
        await camera.start()
        setIsActive(true)
        setError(null)
      } catch (err) {
        setError('ì¹´ë©”ë¼ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.')
        console.error('Camera initialization error:', err)
      }
    }

    if (isStreaming) {
      initializeFaceMesh()
    }

    // ìº”ë²„ìŠ¤ í¬ê¸° ì¡°ì •
    const resizeCanvas = () => {
      if (canvas && video) {
        canvas.width = video.videoWidth || 1280
        canvas.height = video.videoHeight || 720
      }
    }

    video.addEventListener('loadedmetadata', resizeCanvas)
    resizeCanvas()

    return () => {
      video.removeEventListener('loadedmetadata', resizeCanvas)
      if (camera) {
        camera.stop()
      }
      if (faceMesh) {
        faceMesh.close()
      }
      setIsActive(false)
    }
  }, [isStreaming, fps, onFaceDetected])

  const handleStart = async () => {
    try {
      await startCamera()
    } catch (err) {
      setError('ì¹´ë©”ë¼ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
      console.error(err)
    }
  }

  const handleStop = () => {
    stopCamera()
    setIsActive(false)
  }

  return (
    <div className="camera-view">
      <div className="camera-container">
        <video
          ref={videoRef}
          className="camera-video"
          autoPlay
          playsInline
          muted
        />
        <canvas ref={canvasRef} className="camera-canvas" />
        
        {/* ìŒì„± ì¸ì‹ í…ìŠ¤íŠ¸ í‘œì‹œ ì˜ì—­ */}
        {transcript && (
          <div className="speech-transcript">
            <div className="transcript-label">ğŸ¤ ì¸ì‹ëœ ë§:</div>
            <div className="transcript-text">{transcript}</div>
          </div>
        )}
      </div>
      
      {error && <div className="error-message">{error}</div>}
      
      <div className="camera-controls">
        {!isActive ? (
          <button onClick={handleStart} className="btn-primary">
            ì¹´ë©”ë¼ ì‹œì‘
          </button>
        ) : (
          <button onClick={handleStop} className="btn-secondary">
            ì¹´ë©”ë¼ ì¤‘ì§€
          </button>
        )}
      </div>
    </div>
  )
}
