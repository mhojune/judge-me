import { useEffect, useRef, useState } from 'react'
import { useMic } from '../../hooks/useMic'
import { useSpeechRecognition } from '../../hooks/useSpeechRecognition'
import './MicAnalyzer.css'

interface MicAnalyzerProps {
  onAudioData?: (data: {
    volume: number
    frequency: number
    isSpeaking: boolean
  }) => void
  onTranscript?: (transcript: string) => void
  sensitivity?: number
}

/**
 * MicAnalyzer ì»´í¬ë„ŒíŠ¸
 * 
 * ì—­í• :
 * - ì‚¬ìš©ì ë§ˆì´í¬ ì…ë ¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ì„
 * - ìŒì„± ë³¼ë¥¨ ë° ì£¼íŒŒìˆ˜ ë¶„ì„
 * - ë§í•˜ê¸° ìƒíƒœ ê°ì§€
 * - Web Audio APIë¥¼ ì‚¬ìš©í•œ ì˜¤ë””ì˜¤ ì²˜ë¦¬
 */
export default function MicAnalyzer({ 
  onAudioData,
  onTranscript,
  sensitivity = 0.4 // ë” ëœ ë¯¼ê°í•˜ê²Œ ì¡°ì •
}: MicAnalyzerProps) {
  const [isAnalyzing, setIsAnalyzing] = useState(false)
  const [volume, setVolume] = useState(0)
  const [isSpeaking, setIsSpeaking] = useState(false)
  const { startMic, stopMic, isStreaming, stream } = useMic()
  const { transcript, isListening, isSupported, error: speechError, startListening, stopListening } = useSpeechRecognition('ko-KR')
  
  const audioContextRef = useRef<AudioContext | null>(null)
  const analyserRef = useRef<AnalyserNode | null>(null)
  const dataArrayRef = useRef<Uint8Array | null>(null)
  const animationFrameRef = useRef<number | null>(null)
  const onAudioDataRef = useRef(onAudioData)
  const streamRef = useRef<MediaStream | null>(null)
  const lastVolumeRef = useRef(0)
  const lastSpeakingRef = useRef(false)
  const updateCounterRef = useRef(0)
  const noiseLevelRef = useRef<number | null>(null) // ê¸°ë³¸ ë…¸ì´ì¦ˆ ë ˆë²¨
  const noiseSamplesRef = useRef<number[]>([]) // ë…¸ì´ì¦ˆ ìƒ˜í”Œ ìˆ˜ì§‘
  const calibrationFramesRef = useRef(0) // ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í”„ë ˆì„ ì¹´ìš´í„°

  // onAudioData ì½œë°±ì„ refì— ì €ì¥í•˜ì—¬ ì˜ì¡´ì„± ë°°ì—´ ë¬¸ì œ í•´ê²°
  useEffect(() => {
    onAudioDataRef.current = onAudioData
  }, [onAudioData])

  // streamì„ refì— ì €ì¥
  useEffect(() => {
    streamRef.current = stream
  }, [stream])

  useEffect(() => {
    // ìŠ¤íŠ¸ë¦¼ì´ ì—†ê±°ë‚˜ ë¶„ì„ ì¤‘ì´ ì•„ë‹ˆë©´ ì •ë¦¬
    if (!stream || !isAnalyzing || stream.getTracks().length === 0) {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
        animationFrameRef.current = null
      }
      if (audioContextRef.current) {
        audioContextRef.current.close().catch(console.error)
        audioContextRef.current = null
      }
      lastVolumeRef.current = 0
      lastSpeakingRef.current = false
      setVolume(0)
      setIsSpeaking(false)
      updateCounterRef.current = 0
      // ë…¸ì´ì¦ˆ ë ˆë²¨ ë¦¬ì…‹
      noiseLevelRef.current = null
      noiseSamplesRef.current = []
      calibrationFramesRef.current = 0
      return
    }

    const initializeAudio = () => {
      try {
        // ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì •ë¦¬
        if (audioContextRef.current) {
          audioContextRef.current.close().catch(console.error)
          audioContextRef.current = null
        }
        if (animationFrameRef.current) {
          cancelAnimationFrame(animationFrameRef.current)
          animationFrameRef.current = null
        }

        const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
        const analyser = audioContext.createAnalyser()
        
        // ìŠ¤íŠ¸ë¦¼ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        const audioTracks = stream.getAudioTracks()
        if (audioTracks.length === 0 || !audioTracks[0].enabled) {
          console.warn('No active audio tracks')
          audioContext.close().catch(console.error)
          return
        }

        const microphone = audioContext.createMediaStreamSource(stream)
        
        analyser.fftSize = 2048 // ë” ë†’ì€ í•´ìƒë„ë¡œ ë³€ê²½
        analyser.smoothingTimeConstant = 0.3 // ë” ë¹ ë¥¸ ë°˜ì‘
        analyser.minDecibels = -100
        analyser.maxDecibels = 0
        microphone.connect(analyser)
        
        audioContextRef.current = audioContext
        analyserRef.current = analyser
        dataArrayRef.current = new Uint8Array(analyser.frequencyBinCount)
        
        analyzeAudio()
      } catch (err) {
        console.error('Audio initialization error:', err)
        setIsAnalyzing(false)
      }
    }

    const analyzeAudio = () => {
      if (!analyserRef.current || !dataArrayRef.current || !streamRef.current) {
        return
      }

      // ìŠ¤íŠ¸ë¦¼ì´ ì—¬ì „íˆ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
      const tracks = streamRef.current.getAudioTracks()
      if (tracks.length === 0 || !tracks[0].enabled || tracks[0].readyState !== 'live') {
        animationFrameRef.current = requestAnimationFrame(analyzeAudio)
        return
      }

      try {
        // getByteFrequencyDataë¥¼ ì‚¬ìš©í•˜ì—¬ ë³¼ë¥¨ ì¸¡ì •
        analyserRef.current.getByteFrequencyData(dataArrayRef.current)
        
        // ë³¼ë¥¨ ê³„ì‚° - ìµœëŒ€ê°’ê³¼ í‰ê· ì„ ëª¨ë‘ í™œìš©
        let sum = 0
        let maxValue = 0
        let nonZeroCount = 0
        
        for (let i = 0; i < dataArrayRef.current.length; i++) {
          const value = dataArrayRef.current[i]
          sum += value
          if (value > maxValue) {
            maxValue = value
          }
          if (value > 0) {
            nonZeroCount++
          }
        }
        
        // í‰ê·  ê³„ì‚°
        const average = sum / dataArrayRef.current.length
        
        // ìµœëŒ€ê°’ê³¼ í‰ê· ì„ ì¡°í•©í•˜ì—¬ ë³¼ë¥¨ ê³„ì‚°
        const maxNormalized = maxValue / 255
        const avgNormalized = average / 255
        
        // ë³¼ë¥¨ = ìµœëŒ€ê°’(70%) + í‰ê· ê°’(30%) + í™œì„± ì£¼íŒŒìˆ˜ ë¹„ìœ¨(ë³´ë„ˆìŠ¤)
        const activeRatio = nonZeroCount / dataArrayRef.current.length
        const rawVolume = Math.min(1, 
          (maxNormalized * 0.7 + avgNormalized * 0.3) * 3 + activeRatio * 0.08
        )
        
        // ê¸°ë³¸ ë…¸ì´ì¦ˆ ë ˆë²¨ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (ì²˜ìŒ 2ì´ˆ ë™ì•ˆ)
        if (noiseLevelRef.current === null) {
          calibrationFramesRef.current++
          noiseSamplesRef.current.push(rawVolume)
          
          // ì•½ 2ì´ˆ í›„ (60í”„ë ˆì„ ê¸°ì¤€) í‰ê·  ë…¸ì´ì¦ˆ ë ˆë²¨ ê³„ì‚°
          if (calibrationFramesRef.current >= 60) {
            const noiseSum = noiseSamplesRef.current.reduce((a, b) => a + b, 0)
            noiseLevelRef.current = noiseSum / noiseSamplesRef.current.length
            console.log('ê¸°ë³¸ ë…¸ì´ì¦ˆ ë ˆë²¨:', noiseLevelRef.current)
          }
        }
        
        // ë…¸ì´ì¦ˆ ë ˆë²¨ì„ ë¹¼ì„œ ì‹¤ì œ ë³¼ë¥¨ ê³„ì‚°
        let normalizedVolume = rawVolume
        if (noiseLevelRef.current !== null) {
          normalizedVolume = Math.max(0, rawVolume - noiseLevelRef.current)
          // ë…¸ì´ì¦ˆ ì œê±° í›„ ë³¼ë¥¨ì„ ë‹¤ì‹œ ì •ê·œí™” (0-1 ë²”ìœ„)
          const maxPossibleVolume = 1 - noiseLevelRef.current
          if (maxPossibleVolume > 0) {
            normalizedVolume = normalizedVolume / maxPossibleVolume
          }
        }
        
        // ë§í•˜ê¸° ìƒíƒœ ê°ì§€ (ë…¸ì´ì¦ˆ ë ˆë²¨ì„ ê³ ë ¤í•œ threshold)
        const baseThreshold = noiseLevelRef.current !== null 
          ? noiseLevelRef.current + (sensitivity * 0.3)
          : sensitivity * 0.5
        const speakingThreshold = Math.min(0.8, baseThreshold)
        const speaking = normalizedVolume > speakingThreshold || (noiseLevelRef.current !== null && maxValue > noiseLevelRef.current * 255 + 20)
        
        // ìƒíƒœ ì—…ë°ì´íŠ¸ ìµœì í™”: ê°’ì´ ì‹¤ì œë¡œ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ì—…ë°ì´íŠ¸
        // ë³¼ë¥¨ì€ 0.01 ì´ìƒ ì°¨ì´ë‚  ë•Œë§Œ ì—…ë°ì´íŠ¸
        const volumeDiff = Math.abs(normalizedVolume - lastVolumeRef.current)
        if (volumeDiff > 0.01) {
          lastVolumeRef.current = normalizedVolume
          setVolume(normalizedVolume)
        }
        
        // ë§í•˜ê¸° ìƒíƒœëŠ” ì‹¤ì œë¡œ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ì—…ë°ì´íŠ¸
        if (speaking !== lastSpeakingRef.current) {
          lastSpeakingRef.current = speaking
          setIsSpeaking(speaking)
        }
        
        // ì£¼íŒŒìˆ˜ ë¶„ì„
        let maxIndex = 0
        let frequencyMaxValue = 0
        for (let i = 0; i < dataArrayRef.current.length; i++) {
          if (dataArrayRef.current[i] > frequencyMaxValue) {
            frequencyMaxValue = dataArrayRef.current[i]
            maxIndex = i
          }
        }
        const frequency = (maxIndex * audioContextRef.current!.sampleRate) / (2 * analyserRef.current.fftSize)
        
        // onAudioData ì½œë°±ì€ 10í”„ë ˆì„ë§ˆë‹¤ í•œ ë²ˆë§Œ í˜¸ì¶œ (ì•½ 60fps ê¸°ì¤€ìœ¼ë¡œ 6fps)
        updateCounterRef.current++
        if (updateCounterRef.current >= 10) {
          updateCounterRef.current = 0
          onAudioDataRef.current?.({
            volume: normalizedVolume,
            frequency,
            isSpeaking: speaking,
          })
        }

        animationFrameRef.current = requestAnimationFrame(analyzeAudio)
      } catch (err) {
        console.error('Audio analysis error:', err)
        animationFrameRef.current = requestAnimationFrame(analyzeAudio)
      }
    }

    initializeAudio()

    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current)
        animationFrameRef.current = null
      }
      if (audioContextRef.current) {
        audioContextRef.current.close().catch(console.error)
        audioContextRef.current = null
      }
      analyserRef.current = null
      dataArrayRef.current = null
    }
  }, [stream, isAnalyzing, sensitivity])

  // ìŒì„± ì¸ì‹ í…ìŠ¤íŠ¸ë¥¼ ë¶€ëª¨ ì»´í¬ë„ŒíŠ¸ì— ì „ë‹¬
  useEffect(() => {
    if (transcript && onTranscript) {
      onTranscript(transcript)
    }
  }, [transcript, onTranscript])

  const handleStart = async () => {
    try {
      setIsAnalyzing(true)
      await startMic()
      
      // ìŒì„± ì¸ì‹ ì‹œì‘ (ì§€ì›ë˜ëŠ” ê²½ìš°)
      if (isSupported) {
        startListening()
      }
    } catch (err) {
      console.error('Failed to start mic:', err)
      setIsAnalyzing(false)
    }
  }

  const handleStop = () => {
    setIsAnalyzing(false)
    stopMic()
    
    // ìŒì„± ì¸ì‹ ì¤‘ì§€
    if (isListening) {
      stopListening()
    }
  }

  return (
    <div className="mic-analyzer">
      <div className="mic-visualizer">
        <div 
          className="volume-bar"
          style={{ 
            width: `${volume * 100}%`,
            backgroundColor: isSpeaking ? '#4CAF50' : '#2196F3'
          }}
        />
        <div className="volume-indicator">
          {isSpeaking ? 'ğŸ¤ ë§í•˜ëŠ” ì¤‘' : 'ğŸ”‡ ì¡°ìš©í•¨'}
        </div>
      </div>
      
      {speechError && (
        <div className="speech-error">
          {speechError}
        </div>
      )}
      
      {!isSupported && (
        <div className="speech-warning">
          âš ï¸ ìŒì„± ì¸ì‹ì€ Chrome ë˜ëŠ” Edgeì—ì„œë§Œ ì§€ì›ë©ë‹ˆë‹¤.
        </div>
      )}
      
      <div className="mic-controls">
        {!isAnalyzing ? (
          <button onClick={handleStart} className="btn-primary">
            ë§ˆì´í¬ ì‹œì‘
          </button>
        ) : (
          <button onClick={handleStop} className="btn-secondary">
            ë§ˆì´í¬ ì¤‘ì§€
          </button>
        )}
      </div>
    </div>
  )
}
